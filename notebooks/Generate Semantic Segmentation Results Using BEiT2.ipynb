{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6EAEjqrSjwQ",
        "outputId": "6057947e-41a7-429c-e9fb-768d35f75cd7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from joblib import Parallel, delayed\n",
        "import math"
      ],
      "metadata": {
        "id": "aiJJ-FKwTThF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Kaggle-CLI and downlaod dataset"
      ],
      "metadata": {
        "id": "4UppZqYbS1Ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p ~/.kaggle\n",
        "# * Change the line below with the path to your own kaggle api key\n",
        "! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "! chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "EPZZbwUySww0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d weipengzhang/adobe-fivek\n",
        "! unzip adobe-fivek.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHwDZmDqSnK1",
        "outputId": "957e4cad-e0ba-475f-ec0a-95b019b36ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading adobe-fivek.zip to /content\n",
            " 95% 25.1G/26.5G [20:52<01:06, 22.9MB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = Path('./raw')\n",
        "input_imgs = list(img_dir.glob(\"*.jpg\"))"
      ],
      "metadata": {
        "id": "7EEmJ4tRzlve"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# * Resize imgs\n",
        "def resize(img_path):\n",
        "    img = cv2.imread(str(img_path))\n",
        "    img = cv2.resize(img, (512,512), interpolation=cv2.INTER_NEAREST)\n",
        "    cv2.imwrite(str(img_path), img)\n",
        "\n",
        "parallel = Parallel(os.cpu_count(), backend=\"multiprocessing\")\n",
        "parallel(delayed(resize)(img_path) for img_path in tqdm(input_imgs))"
      ],
      "metadata": {
        "id": "mDxGC8Qhmbtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Segmentation"
      ],
      "metadata": {
        "id": "NjUhmFDNI0Jn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build environment"
      ],
      "metadata": {
        "id": "9sr3Nt59KnOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ulz7ReWysZa"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/microsoft/unilm.git\n",
        "! cd unilm/beit2 ; pip install -r requirements.txt\n",
        "\n",
        "! pip install openmim\n",
        "! mim install mmcv-full==1.3.0\n",
        "! pip install scipy timm==0.3.2 mmsegmentation==0.11.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# * Download model weights\n",
        "!wget https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_large_patch16_224_pt1k_ft21ktoade20k.pth\n",
        "# ckpt_path = f'\"/content/drive/MyDrive/CS 7150/project/beitv2_large_patch16_224_pt1k_ft21ktoade20k.pth\"'\n",
        "# ! cp $ckpt_path beitv2_large_patch16_224_pt1k_ft21ktoade20k.pth"
      ],
      "metadata": {
        "id": "ZiPDNGbIzIbP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build model"
      ],
      "metadata": {
        "id": "1xVmVrhJKtnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('unilm/beit2/semantic_segmentation')\n",
        "from backbone import beit\n",
        "from mmseg.apis import init_segmentor, show_result_pyplot"
      ],
      "metadata": {
        "id": "67lfbW1-zWs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b2949b4-2472-4f4a-8f67-9e6ee88750a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex is not installed\n",
            "apex is not installed\n",
            "apex is not installed\n",
            "apex is not installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_file = 'unilm/beit2/semantic_segmentation/configs/beit/upernet/upernet_beit_large_24_512_slide_160k_21ktoade20k.py'\n",
        "checkpoint_file = 'beitv2_large_patch16_224_pt1k_ft21ktoade20k.pth'\n",
        "model = init_segmentor(config_file, checkpoint_file, device='cuda:0')"
      ],
      "metadata": {
        "id": "ASQGBvh2zdHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa3acda2-e073-4b9d-963e-cd3e35b62d6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use load_from_local loader\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model inference"
      ],
      "metadata": {
        "id": "fd0feXRNKx3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seg_dir = Path('./segs')\n",
        "seg_dir.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "V2WYPXgjKhzD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# * Copied from mmseg repo\n",
        "import mmcv\n",
        "import torch\n",
        "from mmcv.parallel import collate, scatter\n",
        "from mmcv.runner import load_checkpoint\n",
        "\n",
        "from mmseg.datasets.pipelines import Compose\n",
        "\n",
        "class LoadImage:\n",
        "    \"\"\"A simple pipeline to load image.\"\"\"\n",
        "\n",
        "    def __call__(self, results):\n",
        "        \"\"\"Call function to load images into results.\n",
        "\n",
        "        Args:\n",
        "            results (dict): A result dict contains the file name\n",
        "                of the image to be read.\n",
        "\n",
        "        Returns:\n",
        "            dict: ``results`` will be returned containing loaded image.\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(results['img'], str):\n",
        "            results['filename'] = results['img']\n",
        "            results['ori_filename'] = results['img']\n",
        "        else:\n",
        "            results['filename'] = None\n",
        "            results['ori_filename'] = None\n",
        "        img = mmcv.imread(results['img'])\n",
        "        results['img'] = img\n",
        "        results['img_shape'] = img.shape\n",
        "        results['ori_shape'] = img.shape\n",
        "        return results\n",
        "\n",
        "def inference_segmentor(model, imgs):\n",
        "    \"\"\"Inference image(s) with the segmentor.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The loaded segmentor.\n",
        "        imgs (str/ndarray or list[str/ndarray]): Either image files or loaded\n",
        "            images.\n",
        "\n",
        "    Returns:\n",
        "        (list[Tensor]): The segmentation result.\n",
        "    \"\"\"\n",
        "    cfg = model.cfg\n",
        "    device = next(model.parameters()).device  # model device\n",
        "    # build the data pipeline\n",
        "    test_pipeline = [LoadImage()] + cfg.data.test.pipeline[1:]\n",
        "    test_pipeline = Compose(test_pipeline)\n",
        "    # prepare data\n",
        "    data = []\n",
        "    imgs = imgs if isinstance(imgs, list) else [imgs]\n",
        "    for img in imgs:\n",
        "        img_data = dict(img=img)\n",
        "        img_data = test_pipeline(img_data)\n",
        "        data.append(img_data)\n",
        "    data = collate(data, samples_per_gpu=len(imgs))\n",
        "    if next(model.parameters()).is_cuda:\n",
        "        # scatter to specified GPU\n",
        "        data = scatter(data, [device])[0]\n",
        "    else:\n",
        "        data['img_metas'] = [i.data[0] for i in data['img_metas']]\n",
        "\n",
        "    # forward the model\n",
        "    with torch.no_grad():\n",
        "        result = model(return_loss=False, rescale=True, **data)\n",
        "    return result"
      ],
      "metadata": {
        "id": "7u2ekg4di5gr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "for i in tqdm(range(math.ceil(len(input_imgs)/batch_size))):\n",
        "    paths = input_imgs[i*batch_size:(i+1)*batch_size]\n",
        "    res = inference_segmentor(model, [str(p) for p in paths])\n",
        "    for j in range(len(paths)):\n",
        "        img = paths[j]\n",
        "        np.save(str(seg_dir / (img.stem+\".npy\")), res[j].astype('uint8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HL6_0RnjG0y",
        "outputId": "73f28f8b-3ab2-4d6d-9dbd-ffae87adc557"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [53:16<00:00,  5.11s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! zip -r segs.zip ./segs\n",
        "! cp segs.zip /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "yX1rQOhAfKkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! mkdir seg_dataset\n",
        "# ! mv segs.zip seg_dataset/segs.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhVEbmynCl9Y",
        "outputId": "6c07adb3-d01f-4978-9dbe-609d8fa27668"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘seg_dataset’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# * Upload to kaggle\n",
        "# ! kaggle datasets init -p seg_dataset\n",
        "# ! kaggle datasets create -p seg_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e9UFFssnsED",
        "outputId": "ee631439-fda9-4139-c333-8eb8fa76c224"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping folder: .ipynb_checkpoints; use '--dir-mode' to upload folders\n",
            "Starting upload for file segs.zip\n",
            "100% 20.0M/20.0M [00:04<00:00, 4.46MB/s]\n",
            "Upload successful: segs.zip (20MB)\n",
            "Your private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/weipengzhang/beit2-adobe5k\n"
          ]
        }
      ]
    }
  ]
}